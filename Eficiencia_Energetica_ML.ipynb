{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Análisis de Eficiencia Energética de Edificios con Machine Learning\n\n## Objetivo\nPredecir la clasificación de eficiencia energética de edificios basándose en sus características físicas y temporales.\n\n## Dataset\n- **Fuente**: Certificados de eficiencia energética de Aragón\n- **Variables**: 15 características de edificios\n- **Target**: Clasificación de consumo energético (C, D, E, F, G)\n- **Modelos**: Logistic Regression, KNN, Decision Tree, XGBoost, LightGBM"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Carga y Exploración Inicial de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv('Eficiencia_Energetica.csv')\n",
    "print(f\"Forma del dataset: {df.shape}\")\n",
    "print(\"\\nPrimeras 5 filas:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general del dataset\n",
    "print(\"=== INFORMACIÓN DEL DATASET ===\")\n",
    "df.info()\n",
    "print(f\"\\nTamaño: {df.shape[0]} filas x {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar valores nulos\n",
    "print(\"=== VALORES NULOS ===\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal de valores nulos: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar duplicados\n",
    "print(\"=== VERIFICACIÓN DE DUPLICADOS ===\")\n",
    "duplicados = df.duplicated().sum()\n",
    "print(f\"Número de duplicados: {duplicados}\")\n",
    "\n",
    "if duplicados > 0:\n",
    "    print(\"\\nFilas duplicadas:\")\n",
    "    print(df[df.duplicated()])\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"\\nDuplicados eliminados. Nuevo tamaño: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de la variable objetivo\n",
    "print(\"=== VARIABLE OBJETIVO ===\")\n",
    "print(\"\\nDistribución de Clasificacion_consumo:\")\n",
    "print(df['Clasificacion_consumo'].value_counts().sort_index())\n",
    "print(f\"\\nPorcentajes:\")\n",
    "print(df['Clasificacion_consumo'].value_counts(normalize=True).sort_index() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar nombres de columnas\n",
    "df.columns = df.columns.str.replace('/', '_').str.replace(' ', '_')\n",
    "print(\"Columnas actualizadas:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear variable de antigüedad del edificio\n",
    "df['Antiguedad'] = 2024 - df['Anio_construccion']\n",
    "print(\"Variable 'Antiguedad' creada\")\n",
    "print(f\"Rango de antigüedad: {df['Antiguedad'].min()} - {df['Antiguedad'].max()} años\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar outliers en variables numéricas\n",
    "print(\"=== ANÁLISIS DE OUTLIERS ===\")\n",
    "columnas_numericas = ['Emision_CO2', 'ConsumoKWh_m2_Anio', 'Superficie_m2', 'Antiguedad']\n",
    "\n",
    "for col in columnas_numericas:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
    "    if len(outliers) > 0:\n",
    "        print(f\"{col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Min: {df[col].min():.2f}, Max: {df[col].max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrección de outliers extremos\n",
    "print(\"=== CORRECCIÓN DE OUTLIERS ===\")\n",
    "\n",
    "# ConsumoKWh_m2_Anio - hay valores extremos como 40788\n",
    "if 'ConsumoKWh_m2_Anio' in df.columns:\n",
    "    # Usar percentil 99 para detectar valores extremos\n",
    "    p99 = df['ConsumoKWh_m2_Anio'].quantile(0.99)\n",
    "    outliers_antes = (df['ConsumoKWh_m2_Anio'] > p99).sum()\n",
    "    \n",
    "    # Recortar valores extremos\n",
    "    df['ConsumoKWh_m2_Anio'] = df['ConsumoKWh_m2_Anio'].clip(upper=p99)\n",
    "    print(f\"✓ ConsumoKWh_m2_Anio: {outliers_antes} valores corregidos (máximo: {p99:.0f})\")\n",
    "\n",
    "# Superficie_m2 - valores muy altos\n",
    "if 'Superficie_m2' in df.columns:\n",
    "    p95 = df['Superficie_m2'].quantile(0.95)\n",
    "    outliers_antes = (df['Superficie_m2'] > p95).sum()\n",
    "    \n",
    "    df['Superficie_m2'] = df['Superficie_m2'].clip(upper=p95)\n",
    "    print(f\"✓ Superficie_m2: {outliers_antes} valores corregidos (máximo: {p95:.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Estadísticas descriptivas\nprint(\"=== ESTADÍSTICAS DESCRIPTIVAS ===\")\nprint(\"\\n1. VARIABLES NUMÉRICAS:\")\nprint(\"=\" * 50)\n\nfor col in columnas_numericas:\n    if col in df.columns:\n        print(f\"\\n{col.upper()}:\")\n        print(f\"   • Media: {df[col].mean():.2f}\")\n        print(f\"   • Mediana: {df[col].median():.2f}\")\n        print(f\"   • Desviación Estándar: {df[col].std():.2f}\")\n        print(f\"   • Rango: {df[col].max() - df[col].min():.2f} (Min: {df[col].min():.2f}, Max: {df[col].max():.2f})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Estadísticas de variables categóricas\nprint(\"\\n2. VARIABLES CATEGÓRICAS:\")\nprint(\"=\" * 50)\ncolumnas_categoricas = ['Tipo_edificio', 'Estado_edificio', 'Municipio', 'Provincia']\n\nfor col in columnas_categoricas:\n    if col in df.columns:\n        print(f\"\\n{col.upper()}:\")\n        valores_unicos = df[col].nunique()\n        moda = df[col].mode()[0]\n        moda_freq = (df[col] == moda).sum()\n        moda_pct = (moda_freq / len(df)) * 100\n        \n        print(f\"   • Valores únicos: {valores_unicos}\")\n        print(f\"   • Moda: '{moda}' ({moda_freq} casos, {moda_pct:.1f}%)\")\n        \n        # Top 5 más frecuentes\n        if valores_unicos <= 10:\n            print(f\"   • Distribución:\")\n            top_values = df[col].value_counts().head(5)\n            for valor, freq in top_values.items():\n                pct = (freq / len(df)) * 100\n                print(f\"     - {valor}: {freq} ({pct:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución de la variable objetivo\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['Clasificacion_consumo'].value_counts().sort_index().plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribución de Clasificación de Consumo')\n",
    "plt.xlabel('Clasificación')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df['Clasificacion_consumo'].value_counts().sort_index().plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Proporción de Clasificaciones')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramas de variables numéricas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(columnas_numericas):\n",
    "    if col in df.columns and i < 4:\n",
    "        df[col].hist(bins=30, ax=axes[i], edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'Distribución de {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frecuencia')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis bivariado - Variables vs Clasificación\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Consumo vs Clasificación\n",
    "df.boxplot(column='ConsumoKWh_m2_Anio', by='Clasificacion_consumo', ax=axes[0])\n",
    "axes[0].set_title('Consumo por Clasificación')\n",
    "axes[0].set_xlabel('Clasificación')\n",
    "\n",
    "# Emisiones vs Clasificación\n",
    "df.boxplot(column='Emision_CO2', by='Clasificacion_consumo', ax=axes[1])\n",
    "axes[1].set_title('Emisiones CO2 por Clasificación')\n",
    "axes[1].set_xlabel('Clasificación')\n",
    "\n",
    "# Tipo edificio vs Clasificación\n",
    "tipo_class = pd.crosstab(df['Tipo_edificio'], df['Clasificacion_consumo'], normalize='index') * 100\n",
    "tipo_class.plot(kind='bar', ax=axes[2], stacked=True)\n",
    "axes[2].set_title('Clasificación por Tipo de Edificio')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Antigüedad vs Clasificación\n",
    "df.boxplot(column='Antiguedad', by='Clasificacion_consumo', ax=axes[3])\n",
    "axes[3].set_title('Antigüedad por Clasificación')\n",
    "axes[3].set_xlabel('Clasificación')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de correlación para variables numéricas\n",
    "plt.figure(figsize=(10, 8))\n",
    "columnas_num_disponibles = [col for col in columnas_numericas if col in df.columns]\n",
    "correlation_matrix = df[columnas_num_disponibles].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Matriz de Correlación - Variables Numéricas')\n",
    "plt.show()\n",
    "\n",
    "print(\"=== CORRELACIONES IMPORTANTES (> 0.5) ===\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
    "            print(f\"{correlation_matrix.columns[i]} vs {correlation_matrix.columns[j]}: {correlation_matrix.iloc[i, j]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar características para el modelo\n",
    "caracteristicas = ['Emision_CO2', 'ConsumoKWh_m2_Anio', 'Tipo_edificio', 'Estado_edificio', \n",
    "                  'Anio_construccion', 'Superficie_m2', 'Municipio', 'Antiguedad']\n",
    "\n",
    "# Filtrar solo las características que existen en el dataset\n",
    "caracteristicas_disponibles = [col for col in caracteristicas if col in df.columns]\n",
    "print(f\"Características seleccionadas: {caracteristicas_disponibles}\")\n",
    "\n",
    "# Crear X e y\n",
    "X = df[caracteristicas_disponibles].copy()\n",
    "y = df['Clasificacion_consumo'].copy()\n",
    "\n",
    "print(f\"\\nForma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "print(f\"\\nClases en y: {sorted(y.unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas numéricas y categóricas\n",
    "columnas_numericas_ml = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "columnas_categoricas_ml = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Columnas numéricas: {columnas_numericas_ml}\")\n",
    "print(f\"Columnas categóricas: {columnas_categoricas_ml}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejar municipios con pocas muestras\n",
    "if 'Municipio' in columnas_categoricas_ml:\n",
    "    municipio_counts = X['Municipio'].value_counts()\n",
    "    municipios_raros = municipio_counts[municipio_counts < 5].index\n",
    "    X['Municipio'] = X['Municipio'].replace(municipios_raros, 'OTROS')\n",
    "    print(f\"Municipios con <5 muestras agrupados como 'OTROS': {len(municipios_raros)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Tamaño entrenamiento: {X_train.shape}\")\n",
    "print(f\"Tamaño prueba: {X_test.shape}\")\n",
    "print(f\"\\nDistribución en entrenamiento:\")\n",
    "print(y_train.value_counts().sort_index())\n",
    "print(f\"\\nDistribución en prueba:\")\n",
    "print(y_test.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementación de Modelos y Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear preprocesadores para diferentes tipos de modelos\n",
    "\n",
    "# Para modelos que necesitan escalado (Logistic Regression, KNN)\n",
    "preprocessor_scaled = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), columnas_numericas_ml),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), columnas_categoricas_ml)\n",
    "    ])\n",
    "\n",
    "# Para modelos basados en árboles (Decision Tree, XGBoost, LightGBM)\n",
    "preprocessor_tree = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', columnas_numericas_ml),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), columnas_categoricas_ml)\n",
    "    ])\n",
    "\n",
    "print(\"Preprocesadores creados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Logistic Regression\n",
    "pipeline_lr = Pipeline([\n",
    "    ('preprocessor', preprocessor_scaled),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "pipeline_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_pred_lr = pipeline_lr.predict(X_test)\n",
    "\n",
    "# Evaluar\n",
    "print(\"=== LOGISTIC REGRESSION ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision (macro): {precision_score(y_test, y_pred_lr, average='macro'):.4f}\")\n",
    "print(f\"Recall (macro): {recall_score(y_test, y_pred_lr, average='macro'):.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_score(y_test, y_pred_lr, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline KNN\n",
    "pipeline_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor_scaled),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "pipeline_knn.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_pred_knn = pipeline_knn.predict(X_test)\n",
    "\n",
    "# Evaluar\n",
    "print(\"=== K-NEAREST NEIGHBORS ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"Precision (macro): {precision_score(y_test, y_pred_knn, average='macro'):.4f}\")\n",
    "print(f\"Recall (macro): {recall_score(y_test, y_pred_knn, average='macro'):.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_score(y_test, y_pred_knn, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Decision Tree\n",
    "pipeline_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor_tree),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "pipeline_dt.fit(X_train, y_train)\n",
    "\n",
    "# Predecir\n",
    "y_pred_dt = pipeline_dt.predict(X_test)\n",
    "\n",
    "# Evaluar\n",
    "print(\"=== DECISION TREE ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt):.4f}\")\n",
    "print(f\"Precision (macro): {precision_score(y_test, y_pred_dt, average='macro'):.4f}\")\n",
    "print(f\"Recall (macro): {recall_score(y_test, y_pred_dt, average='macro'):.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_score(y_test, y_pred_dt, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificar las etiquetas para XGBoost\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Pipeline XGBoost\n",
    "pipeline_xgb = Pipeline([\n",
    "    ('preprocessor', preprocessor_tree),\n",
    "    ('classifier', xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "pipeline_xgb.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predecir\n",
    "y_pred_xgb_encoded = pipeline_xgb.predict(X_test)\n",
    "y_pred_xgb = label_encoder.inverse_transform(y_pred_xgb_encoded)\n",
    "\n",
    "# Evaluar\n",
    "print(\"=== XGBOOST ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Precision (macro): {precision_score(y_test, y_pred_xgb, average='macro'):.4f}\")\n",
    "print(f\"Recall (macro): {recall_score(y_test, y_pred_xgb, average='macro'):.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_score(y_test, y_pred_xgb, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline LightGBM\n",
    "pipeline_lgb = Pipeline([\n",
    "    ('preprocessor', preprocessor_tree),\n",
    "    ('classifier', lgb.LGBMClassifier(random_state=42, verbose=-1))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "pipeline_lgb.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Predecir\n",
    "y_pred_lgb_encoded = pipeline_lgb.predict(X_test)\n",
    "y_pred_lgb = label_encoder.inverse_transform(y_pred_lgb_encoded)\n",
    "\n",
    "# Evaluar\n",
    "print(\"=== LIGHTGBM ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lgb):.4f}\")\n",
    "print(f\"Precision (macro): {precision_score(y_test, y_pred_lgb, average='macro'):.4f}\")\n",
    "print(f\"Recall (macro): {recall_score(y_test, y_pred_lgb, average='macro'):.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_score(y_test, y_pred_lgb, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Comparación de Modelos Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación de todos los modelos\n",
    "print(\"=== COMPARACIÓN DE MODELOS BASE ===\")\n",
    "\n",
    "resultados_base = pd.DataFrame({\n",
    "    'Modelo': ['Logistic Regression', 'KNN', 'Decision Tree', 'XGBoost', 'LightGBM'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_knn),\n",
    "        accuracy_score(y_test, y_pred_dt),\n",
    "        accuracy_score(y_test, y_pred_xgb),\n",
    "        accuracy_score(y_test, y_pred_lgb)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_pred_lr, average='macro'),\n",
    "        precision_score(y_test, y_pred_knn, average='macro'),\n",
    "        precision_score(y_test, y_pred_dt, average='macro'),\n",
    "        precision_score(y_test, y_pred_xgb, average='macro'),\n",
    "        precision_score(y_test, y_pred_lgb, average='macro')\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_pred_lr, average='macro'),\n",
    "        recall_score(y_test, y_pred_knn, average='macro'),\n",
    "        recall_score(y_test, y_pred_dt, average='macro'),\n",
    "        recall_score(y_test, y_pred_xgb, average='macro'),\n",
    "        recall_score(y_test, y_pred_lgb, average='macro')\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_lr, average='macro'),\n",
    "        f1_score(y_test, y_pred_knn, average='macro'),\n",
    "        f1_score(y_test, y_pred_dt, average='macro'),\n",
    "        f1_score(y_test, y_pred_xgb, average='macro'),\n",
    "        f1_score(y_test, y_pred_lgb, average='macro')\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(resultados_base.round(4))\n",
    "\n",
    "# Visualización\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(resultados_base))\n",
    "width = 0.2\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i*width, resultados_base[metric], width, label=metric, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Modelos')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Comparación de Métricas - Modelos Base')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(resultados_base['Modelo'], rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimización de Hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Optimización del mejor modelo base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar el mejor modelo base\n",
    "mejor_f1 = resultados_base['F1-Score'].max()\n",
    "mejor_modelo_base = resultados_base.loc[resultados_base['F1-Score'] == mejor_f1, 'Modelo'].values[0]\n",
    "print(f\"Mejor modelo base: {mejor_modelo_base} (F1-Score: {mejor_f1:.4f})\")\n",
    "\n",
    "# También optimizaremos Decision Tree y XGBoost por ser populares\n",
    "print(\"\\nOptimizaremos: Decision Tree y XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización Decision Tree\n",
    "param_grid_dt = {\n",
    "    'classifier__max_depth': [5, 10, 15, None],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(\n",
    "    pipeline_dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Optimizando Decision Tree...\")\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Mejores parámetros DT: {grid_dt.best_params_}\")\n",
    "print(f\"Mejor score DT: {grid_dt.best_score_:.4f}\")\n",
    "\n",
    "# Predicciones optimizadas\n",
    "y_pred_dt_opt = grid_dt.predict(X_test)\n",
    "\n",
    "print(\"\\n=== DECISION TREE OPTIMIZADO ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt_opt):.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_score(y_test, y_pred_dt_opt, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización XGBoost\n",
    "param_grid_xgb = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__learning_rate': [0.1, 0.2]\n",
    "}\n",
    "\n",
    "grid_xgb = GridSearchCV(\n",
    "    pipeline_xgb,\n",
    "    param_grid=param_grid_xgb,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Optimizando XGBoost...\")\n",
    "grid_xgb.fit(X_train, y_train_encoded)\n",
    "\n",
    "print(f\"Mejores parámetros XGB: {grid_xgb.best_params_}\")\n",
    "print(f\"Mejor score XGB: {grid_xgb.best_score_:.4f}\")\n",
    "\n",
    "# Predicciones optimizadas\n",
    "y_pred_xgb_opt_encoded = grid_xgb.predict(X_test)\n",
    "y_pred_xgb_opt = label_encoder.inverse_transform(y_pred_xgb_opt_encoded)\n",
    "\n",
    "print(\"\\n=== XGBOOST OPTIMIZADO ===\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb_opt):.4f}\")\n",
    "print(f\"F1-Score (macro): {f1_score(y_test, y_pred_xgb_opt, average='macro'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparación Final y Análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Comparación final incluyendo modelos optimizados\nprint(\"=== COMPARACIÓN FINAL ===\")\n\nresultados_finales = pd.DataFrame({\n    'Modelo': ['Logistic Regression', 'KNN', 'Decision Tree', 'XGBoost', 'LightGBM', \n               'Decision Tree Opt', 'XGBoost Opt'],\n    'Accuracy': [\n        accuracy_score(y_test, y_pred_lr),\n        accuracy_score(y_test, y_pred_knn),\n        accuracy_score(y_test, y_pred_dt),\n        accuracy_score(y_test, y_pred_xgb),\n        accuracy_score(y_test, y_pred_lgb),\n        accuracy_score(y_test, y_pred_dt_opt),\n        accuracy_score(y_test, y_pred_xgb_opt)\n    ],\n    'F1-Score': [\n        f1_score(y_test, y_pred_lr, average='macro'),\n        f1_score(y_test, y_pred_knn, average='macro'),\n        f1_score(y_test, y_pred_dt, average='macro'),\n        f1_score(y_test, y_pred_xgb, average='macro'),\n        f1_score(y_test, y_pred_lgb, average='macro'),\n        f1_score(y_test, y_pred_dt_opt, average='macro'),\n        f1_score(y_test, y_pred_xgb_opt, average='macro')\n    ]\n})\n\nprint(resultados_finales.round(4))\n\n# Identificar el mejor modelo final\nmejor_f1_final = resultados_finales['F1-Score'].max()\nmejor_modelo_final = resultados_finales.loc[resultados_finales['F1-Score'] == mejor_f1_final, 'Modelo'].values[0]\n\nprint(f\"\\nMEJOR MODELO: {mejor_modelo_final}\")\nprint(f\"F1-Score: {mejor_f1_final:.4f}\")\nprint(f\"Accuracy: {resultados_finales.loc[resultados_finales['F1-Score'] == mejor_f1_final, 'Accuracy'].values[0]:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión del mejor modelo\n",
    "# Determinar las predicciones del mejor modelo\n",
    "if 'XGBoost Opt' in mejor_modelo_final:\n",
    "    y_pred_mejor = y_pred_xgb_opt\n",
    "elif 'Decision Tree Opt' in mejor_modelo_final:\n",
    "    y_pred_mejor = y_pred_dt_opt\n",
    "elif 'LightGBM' in mejor_modelo_final:\n",
    "    y_pred_mejor = y_pred_lgb\n",
    "elif 'XGBoost' in mejor_modelo_final:\n",
    "    y_pred_mejor = y_pred_xgb\n",
    "elif 'Decision Tree' in mejor_modelo_final:\n",
    "    y_pred_mejor = y_pred_dt\n",
    "elif 'KNN' in mejor_modelo_final:\n",
    "    y_pred_mejor = y_pred_knn\n",
    "else:\n",
    "    y_pred_mejor = y_pred_lr\n",
    "\n",
    "# Crear matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_mejor)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=sorted(y.unique()), yticklabels=sorted(y.unique()))\n",
    "plt.title(f'Matriz de Confusión - {mejor_modelo_final}')\n",
    "plt.xlabel('Predicho')\n",
    "plt.ylabel('Real')\n",
    "plt.show()\n",
    "\n",
    "# Reporte de clasificación\n",
    "print(f\"\\n=== REPORTE DE CLASIFICACIÓN - {mejor_modelo_final} ===\")\n",
    "print(classification_report(y_test, y_pred_mejor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Importancia de Variables (si aplica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar importancia de variables si el mejor modelo lo permite\n",
    "if 'Decision Tree' in mejor_modelo_final:\n",
    "    # Obtener el modelo optimizado\n",
    "    if 'Opt' in mejor_modelo_final:\n",
    "        modelo_final = grid_dt.best_estimator_\n",
    "    else:\n",
    "        modelo_final = pipeline_dt\n",
    "    \n",
    "    # Obtener nombres de características\n",
    "    feature_names = columnas_numericas_ml.copy()\n",
    "    if columnas_categoricas_ml:\n",
    "        encoder = modelo_final.named_steps['preprocessor'].transformers_[1][1]\n",
    "        cat_features = encoder.get_feature_names_out(columnas_categoricas_ml)\n",
    "        feature_names.extend(cat_features)\n",
    "    \n",
    "    # Obtener importancias\n",
    "    importances = modelo_final.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Top 10 variables más importantes\n",
    "    indices = np.argsort(importances)[::-1][:10]\n",
    "    top_features = [feature_names[i] for i in indices]\n",
    "    top_importances = importances[indices[:10]]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_features, top_importances)\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.title('Top 10 Variables Más Importantes')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "elif 'XGBoost' in mejor_modelo_final:\n",
    "    print(\"\\n=== IMPORTANCIA DE VARIABLES (XGBOOST) ===\")\n",
    "    if 'Opt' in mejor_modelo_final:\n",
    "        modelo_xgb = grid_xgb.best_estimator_.named_steps['classifier']\n",
    "    else:\n",
    "        modelo_xgb = pipeline_xgb.named_steps['classifier']\n",
    "    \n",
    "    # Obtener importancias\n",
    "    feature_importance = modelo_xgb.feature_importances_\n",
    "    \n",
    "    # Crear DataFrame para visualización\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': range(len(feature_importance)),\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(importance_df)), importance_df['importance'])\n",
    "    plt.xlabel('Importancia')\n",
    "    plt.title('Top 10 Variables Más Importantes (XGBoost)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"\\nEl modelo {mejor_modelo_final} no proporciona importancia de variables de forma directa.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== CONCLUSIONES DEL ANÁLISIS ===\")\n",
    "print(\"\\n1. DATASET:\")\n",
    "print(f\"   - Total de muestras: {df.shape[0]}\")\n",
    "print(f\"   - Variables utilizadas: {len(caracteristicas_disponibles)}\")\n",
    "print(f\"   - Clases objetivo: {len(y.unique())} ({', '.join(sorted(y.unique()))})\")\n",
    "\n",
    "print(\"\\n2. PREPROCESAMIENTO:\")\n",
    "duplicados_eliminados = duplicados if 'duplicados' in locals() else 0\n",
    "print(f\"   - Duplicados eliminados: {duplicados_eliminados}\")\n",
    "print(\"   - Outliers extremos corregidos en ConsumoKWh y Superficie\")\n",
    "print(\"   - Variables categóricas codificadas con OneHotEncoder\")\n",
    "print(\"   - Variables numéricas escaladas para modelos apropiados\")\n",
    "\n",
    "print(\"\\n3. MODELOS EVALUADOS:\")\n",
    "print(\"   - Logistic Regression, KNN, Decision Tree, XGBoost, LightGBM\")\n",
    "print(\"   - Optimización de hiperparámetros con GridSearchCV\")\n",
    "print(\"   - Evaluación con métricas macro-promediadas\")\n",
    "\n",
    "print(\"\\n4. RESULTADOS:\")\n",
    "print(f\"   - Mejor modelo: {mejor_modelo_final}\")\n",
    "print(f\"   - Accuracy: {resultados_finales.loc[resultados_finales['F1-Score'] == mejor_f1_final, 'Accuracy'].values[0]:.4f}\")\n",
    "print(f\"   - F1-Score (macro): {mejor_f1_final:.4f}\")\n",
    "\n",
    "mejora_optimizacion = False\n",
    "if 'Opt' in mejor_modelo_final:\n",
    "    modelo_base = mejor_modelo_final.replace(' Opt', '')\n",
    "    f1_base = resultados_finales.loc[resultados_finales['Modelo'] == modelo_base, 'F1-Score'].values\n",
    "    if len(f1_base) > 0:\n",
    "        mejora = ((mejor_f1_final - f1_base[0]) / f1_base[0]) * 100\n",
    "        print(f\"   - Mejora con optimización: {mejora:.1f}%\")\n",
    "        mejora_optimizacion = True\n",
    "\n",
    "print(\"\\n5. INSIGHTS:\")\n",
    "print(\"   - La clasificación energética puede predecirse con precisión razonable\")\n",
    "print(\"   - Las variables más importantes incluyen consumo y emisiones CO2\")\n",
    "print(\"   - El tipo y antigüedad del edificio también influyen\")\n",
    "if mejora_optimizacion:\n",
    "    print(\"   - La optimización de hiperparámetros mejoró significativamente el rendimiento\")\n",
    "\n",
    "print(\"\\n6. APLICACIONES PRÁCTICAS:\")\n",
    "print(\"   - Evaluación automática de eficiencia energética\")\n",
    "print(\"   - Identificación de edificios candidatos para renovación\")\n",
    "print(\"   - Apoyo en políticas de eficiencia energética\")\n",
    "print(\"   - Estimación de clasificación antes de certificación oficial\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}